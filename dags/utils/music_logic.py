import pandas as pd
import os
import re
import json
import traceback
import numpy as np
import gspread
import logging
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io

logger = logging.getLogger("airflow.task")

def _get_files_recursive(service, parent_id):
    found_files = []
    page_token = None
    while True:
        query = f"'{parent_id}' in parents and trashed = false"
        response = service.files().list(q=query, fields="nextPageToken, files(id, name, mimeType)", pageToken=page_token).execute()
        items = response.get('files', [])
        for item in items:
            if item['mimeType'] == 'application/vnd.google-apps.folder':
                found_files.extend(_get_files_recursive(service, item['id']))
            elif 'spreadsheet' in item['mimeType'] or item['name'].endswith('.xlsx'):
                found_files.append(item)
        page_token = response.get('nextPageToken')
        if not page_token: break
    return found_files

def download_files_from_drive(drive_folder_id, key_path, data_dir, scope):
    logger.info(f"ğŸ“¥ êµ¬ê¸€ ë“œë¼ì´ë¸Œ íƒìƒ‰ ì‹œì‘ (Root ID: {drive_folder_id})")
    if not os.path.exists(key_path): raise FileNotFoundError(f"ì¸ì¦ í‚¤ ì—†ìŒ")
    
    creds = ServiceAccountCredentials.from_json_keyfile_name(key_path, scope)
    service = build('drive', 'v3', credentials=creds)
    
    target_files = _get_files_recursive(service, drive_folder_id)
    if not target_files: return

    if not os.path.exists(data_dir): os.makedirs(data_dir)
    for f in os.listdir(data_dir): os.remove(os.path.join(data_dir, f))
    
    for file in target_files:
        safe_name = f"{os.path.splitext(file['name'])[0]}_{file['id'][-6:]}.xlsx"
        try:
            req = service.files().export_media(fileId=file['id'], mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet') if file['mimeType'] == 'application/vnd.google-apps.spreadsheet' else service.files().get_media(fileId=file['id'])
            fh = io.FileIO(os.path.join(data_dir, safe_name), 'wb')
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done: _, done = downloader.next_chunk()
        except: pass
    logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(target_files)}ê±´")

# -----------------------------------------------------------------------------
# [ì—…ê·¸ë ˆì´ë“œ] ì •ë ¬ ì˜µì…˜(sort_by_col_sum) ì¶”ê°€
# -----------------------------------------------------------------------------
def _finalize_dataframe(df_list, index_name="ê¸°ì¤€ë…„ì›”", sort_by_col_sum=False):
    if not df_list: return None
    
    # 1. ë³‘í•©
    final_df = pd.concat(df_list, axis=0, sort=False).fillna(0)
    
    # [NEW] ì»¬ëŸ¼ ì •ë ¬ (ì´ì•¡ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    if sort_by_col_sum:
        # ìˆ«ì ë°ì´í„°ë§Œ ë”í•´ì„œ ìˆœìœ„ ë§¤ê¸°ê¸°
        col_sums = final_df.select_dtypes(include=[np.number]).sum()
        sorted_cols = col_sums.sort_values(ascending=False).index.tolist()
        # ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ë°°ì¹˜
        final_df = final_df[sorted_cols]

    final_df.index.name = index_name
    final_df = final_df.reset_index()
    
    # 2. ê°€ë¡œ í•©ê³„ (Row Sum)
    num_cols = final_df.select_dtypes(include=['float', 'int']).columns
    final_df['í•©ê³„'] = final_df[num_cols].sum(axis=1)
    
    # 3. ì„¸ë¡œ í•©ê³„ (Column Sum) ë° ì •ë ¬
    final_df = final_df.sort_values([index_name], ascending=[True])
    
    sum_series = final_df.select_dtypes(include=[np.number]).sum()
    sum_row = pd.DataFrame(sum_series).T
    sum_row[index_name] = 'ì´ê³„'
    
    final_df = pd.concat([final_df, sum_row], ignore_index=True)
    final_df = final_df.replace({np.nan: '', np.inf: 0, -np.inf: 0})
    
    return final_df

def process_music_settlement(data_dir, key_path, spreadsheet_id, scope):
    logger.info("ğŸš€ ë°ì´í„° ì§‘ê³„ ì‹œì‘ (ì•¨ë²”ë³„ + í”Œë«í¼ë³„ ì •ë ¬ ì ìš©)")
    files = [f for f in os.listdir(data_dir) if f.endswith('.xlsx')]
    if not files: return

    all_album_data = []
    all_platform_data = [] 
    
    for filename in files:
        try:
            date_match = re.search(r'(20\d{4})', filename)
            base_date = date_match.group(1) if date_match else None
            if not base_date: continue

            sheets_dict = pd.read_excel(os.path.join(data_dir, filename), sheet_name=None, header=None)
            target_df = None
            for _, df in sheets_dict.items():
                if len(df) < 2: continue
                if df.astype(str).apply(lambda x: x.str.contains("ì„œë¹„ìŠ¤ëª…")).any().any():
                    target_df = df; break
            
            if target_df is None: continue
            
            start_index = None
            for idx, row in target_df.iterrows():
                if row.astype(str).str.contains("ì„œë¹„ìŠ¤ëª…").any():
                    start_index = idx; break
            
            new_header = target_df.iloc[start_index]
            df = target_df[start_index + 1:].copy()
            df.columns = new_header
            
            df.columns = df.columns.astype(str).str.strip()
            rename_map = {"ê³¡ëª…": "ì•¨ë²”ëª…", "ê¸°íšì‚¬ì •ì‚°ê¸ˆì•¡": "ì •ì‚°ê¸ˆì•¡"}
            df.rename(columns=rename_map, inplace=True)
            df = df.loc[:, ~df.columns.duplicated()]

            if "ì •ì‚°ê¸ˆì•¡" not in df.columns: continue
            
            df["ì •ì‚°ê¸ˆì•¡"] = pd.to_numeric(df["ì •ì‚°ê¸ˆì•¡"].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

            # 1. ì•¨ë²”ë³„ í”¼ë²— (ê¸°ì¡´ëŒ€ë¡œ)
            if "ì•¨ë²”ëª…" in df.columns:
                pivot_album = df.pivot_table(index=[pd.Index([base_date]*len(df), name="ê¸°ì¤€ë…„ì›”")], columns="ì•¨ë²”ëª…", values="ì •ì‚°ê¸ˆì•¡", aggfunc='sum').fillna(0)
                all_album_data.append(pivot_album)

            # 2. í”Œë«í¼ë³„ í”¼ë²—
            if "ì„œë¹„ìŠ¤ëª…" in df.columns:
                pivot_platform = df.pivot_table(index=[pd.Index([base_date]*len(df), name="ê¸°ì¤€ë…„ì›”")], columns="ì„œë¹„ìŠ¤ëª…", values="ì •ì‚°ê¸ˆì•¡", aggfunc='sum').fillna(0)
                all_platform_data.append(pivot_platform)

        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬({filename}): {e}")
            continue

    if not all_album_data:
        logger.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # [ì ìš©] ì•¨ë²”ì€ ì •ë ¬ X, í”Œë«í¼ì€ ì •ë ¬ O (sort_by_col_sum=True)
    final_album_df = _finalize_dataframe(all_album_data, sort_by_col_sum=False)
    final_platform_df = _finalize_dataframe(all_platform_data, sort_by_col_sum=True)

    # ë°ì´í„° ì¡°ë¦½
    upload_data = [final_album_df.columns.values.tolist()] + final_album_df.values.tolist()
    upload_data.append([]) 
    upload_data.append([]) 
    upload_data.append(["[ì°¸ê³ ] í”Œë«í¼ë³„ ì›”ë³„ ìˆ˜ìµ í˜„í™©"]) 
    
    if final_platform_df is not None:
        upload_data.append(final_platform_df.columns.values.tolist())
        upload_data.extend(final_platform_df.values.tolist())

    # JSON ì§ë ¬í™” ì²´í¬
    try: json.dumps(upload_data)
    except TypeError:
        new_upload = []
        for row in upload_data: new_upload.append([str(x) for x in row])
        upload_data = new_upload

    try:
        logger.info(f"â˜ï¸ êµ¬ê¸€ ì‹œíŠ¸(ID: {spreadsheet_id}) ì—…ë¡œë“œ ì¤‘...")
        creds = ServiceAccountCredentials.from_json_keyfile_name(key_path, scope)
        client = gspread.authorize(creds)
        sh = client.open_by_key(spreadsheet_id)
        worksheet = sh.get_worksheet(0)
        worksheet.clear()
        sh.values_update(f"'{worksheet.title}'!A1", params={'valueInputOption': 'USER_ENTERED'}, body={'values': upload_data})
        logger.info(f"ğŸ‰ ì—…ë¡œë“œ ì„±ê³µ!")

    except Exception as e:
        if "<Response [200]>" in str(e): logger.info("ğŸ‰ ì„±ê³µ (200 OK)")
        else: logger.error(f"âŒ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}")

# -----------------------------------------------------------------------------
# [ì—…ê·¸ë ˆì´ë“œ] ë””ìì¸ í•¨ìˆ˜: í—¤ë”ì™€ í•©ê³„ í–‰ì„ 'ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ì„œ' ì ìš©
# -----------------------------------------------------------------------------
def style_google_sheet(key_path, spreadsheet_id, scope):
    logger.info(f"ğŸ¨ ìŠ¤ë§ˆíŠ¸ ë””ìì¸ ì ìš© ì‹œì‘")
    if not os.path.exists(key_path): return

    try:
        creds = ServiceAccountCredentials.from_json_keyfile_name(key_path, scope)
        client = gspread.authorize(creds)
        sh = client.open_by_key(spreadsheet_id)
        worksheet = sh.get_worksheet(0)
        all_values = worksheet.get_all_values()
        if not all_values: return
        
        last_row = len(all_values)
        last_col = len(all_values[0])
        
        requests = [
            # 1. í—¤ë” ê³ ì •
            {"updateSheetProperties": {"properties": {"sheetId": worksheet.id, "gridProperties": {"frozenRowCount": 1}}, "fields": "gridProperties.frozenRowCount"}},
            # 2. ì „ì²´ ê°€ìš´ë° ì •ë ¬
            {"repeatCell": {"range": {"sheetId": worksheet.id, "startRowIndex": 1, "endRowIndex": last_row}, "cell": {"userEnteredFormat": {"horizontalAlignment": "CENTER"}}, "fields": "userEnteredFormat(horizontalAlignment)"}},
            # 3. ìˆ«ì(ì½¤ë§ˆ) + ìš°ì¸¡ ì •ë ¬ (Bì—´ ì´í›„)
            {"repeatCell": {"range": {"sheetId": worksheet.id, "startRowIndex": 1, "endRowIndex": last_row, "startColumnIndex": 1, "endColumnIndex": last_col}, "cell": {"userEnteredFormat": {"numberFormat": {"type": "NUMBER", "pattern": "#,##0"}, "horizontalAlignment": "RIGHT"}}, "fields": "userEnteredFormat(numberFormat,horizontalAlignment)"}},
        ]

        # ---------------------------------------------------------------------
        # [NEW] ë˜‘ë˜‘í•œ í–‰ ì°¾ê¸° ë¡œì§
        # ---------------------------------------------------------------------
        header_style = {"userEnteredFormat": {"textFormat": {"bold": True}, "horizontalAlignment": "CENTER", "backgroundColor": {"red": 0.9, "green": 0.9, "blue": 0.9}}}
        total_style = {"userEnteredFormat": {"textFormat": {"bold": True}, "backgroundColor": {"red": 0.85, "green": 0.85, "blue": 0.85}}}

        # 1. ì²« ë²ˆì§¸ ì¤„(ë¬´ì¡°ê±´ í—¤ë”)
        requests.append({"repeatCell": {"range": {"sheetId": worksheet.id, "startRowIndex": 0, "endRowIndex": 1}, "cell": header_style, "fields": "userEnteredFormat(textFormat,horizontalAlignment,backgroundColor)"}})

        # 2. ë‚´ìš© ë³´ë©´ì„œ 'ì¶”ê°€ í—¤ë”'ì™€ 'ì´ê³„' ì°¾ê¸°
        for i, row in enumerate(all_values):
            if not row: continue
            
            # (1) í”Œë«í¼ë³„ í‘œ ì œëª© ë°”ë¡œ ë‹¤ìŒ ì¤„ = í—¤ë”
            if row[0].startswith("[ì°¸ê³ ]"):
                # ì œëª© ì¤„ ë°”ë¡œ ë‹¤ìŒ(i+1)ì´ í—¤ë”ì„
                header_idx = i + 1
                if header_idx < last_row:
                    requests.append({"repeatCell": {"range": {"sheetId": worksheet.id, "startRowIndex": header_idx, "endRowIndex": header_idx+1}, "cell": header_style, "fields": "userEnteredFormat(textFormat,horizontalAlignment,backgroundColor)"}})
            
            # (2) 'ì´ê³„'ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ = í•©ê³„ í–‰ (íšŒìƒ‰ ê°•ì¡°)
            if row[0] == "ì´ê³„":
                requests.append({"repeatCell": {"range": {"sheetId": worksheet.id, "startRowIndex": i, "endRowIndex": i+1}, "cell": total_style, "fields": "userEnteredFormat(textFormat,backgroundColor)"}})

        # ---------------------------------------------------------------------

        # ë„ˆë¹„ ìë™ ë§ì¶¤
        df = pd.DataFrame(all_values)
        for col_idx in range(len(df.columns)):
            max_len = df[col_idx].astype(str).map(len).max()
            pixel_width = max(50, min(int(max_len * 12 + 30), 400))
            requests.append({"updateDimensionProperties": {"range": {"sheetId": worksheet.id, "dimension": "COLUMNS", "startIndex": col_idx, "endIndex": col_idx + 1}, "properties": {"pixelSize": pixel_width}, "fields": "pixelSize"}})

        sh.batch_update({"requests": requests})
        logger.info("âœ¨ ìŠ¤ë§ˆíŠ¸ ë””ìì¸ ì ìš© ì™„ë£Œ!")

    except Exception as e: logger.error(f"ë””ìì¸ ì‹¤íŒ¨: {e}")

# -----------------------------------------------------------------------------
# [NEW] íŒŒì¼ ë³€ê²½ ê°ì§€ (ShortCircuitìš©)
# -----------------------------------------------------------------------------
def check_drive_changes(drive_folder_id, key_path, scope, data_dir):
    logger.info("ğŸ‘€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë³€ê²½ì‚¬í•­ ê°ì§€ ì¤‘...")
    
    # 1. ìƒíƒœë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê³¼ê±°ì˜ íŒŒì¼ ëª©ë¡ì„ ê¸°ì–µí•˜ëŠ” ìˆ˜ì²©)
    history_file = os.path.join(data_dir, "drive_history.json")
    
    # 2. í˜„ì¬ ë“œë¼ì´ë¸Œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if not os.path.exists(key_path): return False
    creds = ServiceAccountCredentials.from_json_keyfile_name(key_path, scope)
    service = build('drive', 'v3', credentials=creds)
    current_files = _get_files_recursive(service, drive_folder_id)
    
    # ë¹„êµë¥¼ ìœ„í•´ íŒŒì¼ IDë§Œ ì¶”ì¶œí•´ì„œ ì •ë ¬ (Setì´ë‚˜ List)
    current_ids = sorted([f['id'] for f in current_files])
    
    # 3. ê³¼ê±° ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    last_ids = []
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r') as f:
                last_ids = json.load(f)
                # JSON ë¡œë“œ ì‹œ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš° ëŒ€ë¹„
                if not isinstance(last_ids, list): last_ids = []
                last_ids = sorted(last_ids)
        except:
            last_ids = []

    # 4. ë¹„êµ (í˜„ì¬ ëª©ë¡ vs ê³¼ê±° ëª©ë¡)
    if current_ids == last_ids:
        logger.info("ğŸ’¤ ë³€ê²½ì‚¬í•­ ì—†ìŒ. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False  # ë’¤ì— ì˜¤ëŠ” íƒœìŠ¤í¬ë“¤ ëª¨ë‘ Skip!
    else:
        logger.info(f"âœ¨ ë³€ê²½ ê°ì§€! (íŒŒì¼ ìˆ˜: {len(last_ids)} -> {len(current_ids)})")
        
        # [ì¤‘ìš”] ë³€ê²½ì‚¬í•­ì´ í™•ì¸ë˜ì—ˆìœ¼ë‹ˆ, í˜„ì¬ ìƒíƒœë¥¼ ìˆ˜ì²©ì— ê¸°ë¡í•´ë‘¡ë‹ˆë‹¤.
        # (ê·¸ë˜ì•¼ ë‹¤ìŒë²ˆ ì‹¤í–‰ ë•Œ ë˜ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)
        with open(history_file, 'w') as f:
            json.dump(current_ids, f)
            
        return True   # ì‘ì—… ì§„í–‰ì‹œì¼œ!